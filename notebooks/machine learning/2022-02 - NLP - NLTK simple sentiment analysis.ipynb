{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2841bf94-f082-4c4b-9b2c-664eda7c9d92",
   "metadata": {},
   "source": [
    "# NLP - English tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "563f4438-edce-4b81-afc7-235d0709d452",
   "metadata": {},
   "source": [
    "## Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6f5fe283-dfef-45e2-9076-b824eca52da1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.classify import NaiveBayesClassifier\n",
    "from nltk.corpus import subjectivity\n",
    "from nltk.sentiment import SentimentAnalyzer\n",
    "from nltk.sentiment.util import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "240f3306-ee5e-4cdd-83c0-c53f64996dec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package subjectivity to\n",
      "[nltk_data]     /home/nelth/nltk_data...\n",
      "[nltk_data]   Package subjectivity is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/nelth/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /home/nelth/nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('subjectivity')\n",
    "nltk.download('punkt')\n",
    "nltk.download('vader_lexicon')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e6eac4f4-a797-4493-876c-ee0f7fa6f35b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 100)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_instances = 100\n",
    "subj_docs = [(sent, 'subj') for sent in subjectivity.sents(categories='subj')[:n_instances]]\n",
    "obj_docs = [(sent, 'obj') for sent in subjectivity.sents(categories='obj')[:n_instances]]\n",
    "len(subj_docs), len(obj_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da3556f0-fca6-4429-9ec1-405934b2b9d6",
   "metadata": {},
   "source": [
    "> Each document is represented by a tuple (sentence, label). The sentence is tokenized, so it is represented by a list of strings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d3e3e608-2c5a-4574-880a-09ba4aef0ead",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(['smart',\n",
       "   'and',\n",
       "   'alert',\n",
       "   ',',\n",
       "   'thirteen',\n",
       "   'conversations',\n",
       "   'about',\n",
       "   'one',\n",
       "   'thing',\n",
       "   'is',\n",
       "   'a',\n",
       "   'small',\n",
       "   'gem',\n",
       "   '.'],\n",
       "  'subj'),\n",
       " (['color',\n",
       "   ',',\n",
       "   'musical',\n",
       "   'bounce',\n",
       "   'and',\n",
       "   'warm',\n",
       "   'seas',\n",
       "   'lapping',\n",
       "   'on',\n",
       "   'island',\n",
       "   'shores',\n",
       "   '.',\n",
       "   'and',\n",
       "   'just',\n",
       "   'enough',\n",
       "   'science',\n",
       "   'to',\n",
       "   'send',\n",
       "   'you',\n",
       "   'home',\n",
       "   'thinking',\n",
       "   '.'],\n",
       "  'subj'),\n",
       " (['it',\n",
       "   'is',\n",
       "   'not',\n",
       "   'a',\n",
       "   'mass-market',\n",
       "   'entertainment',\n",
       "   'but',\n",
       "   'an',\n",
       "   'uncompromising',\n",
       "   'attempt',\n",
       "   'by',\n",
       "   'one',\n",
       "   'artist',\n",
       "   'to',\n",
       "   'think',\n",
       "   'about',\n",
       "   'another',\n",
       "   '.'],\n",
       "  'subj')]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subj_docs[0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f05f8e02-611f-4ab1-bc85-fa17ed3be37b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(['the',\n",
       "   'movie',\n",
       "   'begins',\n",
       "   'in',\n",
       "   'the',\n",
       "   'past',\n",
       "   'where',\n",
       "   'a',\n",
       "   'young',\n",
       "   'boy',\n",
       "   'named',\n",
       "   'sam',\n",
       "   'attempts',\n",
       "   'to',\n",
       "   'save',\n",
       "   'celebi',\n",
       "   'from',\n",
       "   'a',\n",
       "   'hunter',\n",
       "   '.'],\n",
       "  'obj'),\n",
       " (['emerging',\n",
       "   'from',\n",
       "   'the',\n",
       "   'human',\n",
       "   'psyche',\n",
       "   'and',\n",
       "   'showing',\n",
       "   'characteristics',\n",
       "   'of',\n",
       "   'abstract',\n",
       "   'expressionism',\n",
       "   ',',\n",
       "   'minimalism',\n",
       "   'and',\n",
       "   'russian',\n",
       "   'constructivism',\n",
       "   ',',\n",
       "   'graffiti',\n",
       "   'removal',\n",
       "   'has',\n",
       "   'secured',\n",
       "   'its',\n",
       "   'place',\n",
       "   'in',\n",
       "   'the',\n",
       "   'history',\n",
       "   'of',\n",
       "   'modern',\n",
       "   'art',\n",
       "   'while',\n",
       "   'being',\n",
       "   'created',\n",
       "   'by',\n",
       "   'artists',\n",
       "   'who',\n",
       "   'are',\n",
       "   'unconscious',\n",
       "   'of',\n",
       "   'their',\n",
       "   'artistic',\n",
       "   'achievements',\n",
       "   '.'],\n",
       "  'obj'),\n",
       " (['spurning',\n",
       "   'her',\n",
       "   \"mother's\",\n",
       "   'insistence',\n",
       "   'that',\n",
       "   'she',\n",
       "   'get',\n",
       "   'on',\n",
       "   'with',\n",
       "   'her',\n",
       "   'life',\n",
       "   ',',\n",
       "   'mary',\n",
       "   'is',\n",
       "   'thrown',\n",
       "   'out',\n",
       "   'of',\n",
       "   'the',\n",
       "   'house',\n",
       "   ',',\n",
       "   'rejected',\n",
       "   'by',\n",
       "   'joe',\n",
       "   ',',\n",
       "   'and',\n",
       "   'expelled',\n",
       "   'from',\n",
       "   'school',\n",
       "   'as',\n",
       "   'she',\n",
       "   'grows',\n",
       "   'larger',\n",
       "   'with',\n",
       "   'child',\n",
       "   '.'],\n",
       "  'obj')]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obj_docs[0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b7a2995-8450-408b-b5d4-6de43fc8435f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "515c93a7-40ea-4c52-a3dc-73db7af6b7a6",
   "metadata": {},
   "source": [
    "## Train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1469e2ff-5b30-436b-b72c-de1cd753625c",
   "metadata": {},
   "source": [
    "> We separately split subjective and objective instances to keep a balanced uniform class distribution in both train and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "af485f4f-f311-4b30-9c20-49bdf403d9d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_subj_docs = subj_docs[:80]\n",
    "test_subj_docs = subj_docs[80:100]\n",
    "train_obj_docs = obj_docs[:80]\n",
    "test_obj_docs = obj_docs[80:100]\n",
    "training_docs = train_subj_docs+train_obj_docs\n",
    "testing_docs = test_subj_docs+test_obj_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06fd2d12-2c04-45a5-b808-d5276d0adef8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "04a4da84-09de-4ccb-b6e5-7a241d5bf08d",
   "metadata": {},
   "source": [
    "## Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "135de1a2-f1ff-4907-ba2c-6a66b761a417",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentim_analyzer = SentimentAnalyzer()\n",
    "all_words_neg = sentim_analyzer.all_words([mark_negation(doc) for doc in training_docs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ba92cff9-40c5-49e0-9a65-73aecd887fd8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['smart',\n",
       " 'and',\n",
       " 'alert',\n",
       " ',',\n",
       " 'thirteen',\n",
       " 'conversations',\n",
       " 'about',\n",
       " 'one',\n",
       " 'thing',\n",
       " 'is']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_words_neg[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f745672-c928-452c-9afc-37be8f73dc88",
   "metadata": {},
   "source": [
    "> We use simple unigram word features, handling negation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "6f149a70-40e2-4d4c-8586-86b35c25d8aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of unigram feats: 83\n",
      "['.', 'the', ',', 'a', 'and', 'of', 'to', 'is', 'in', 'with', 'it', 'that', 'his', 'on', 'for', 'an', 'who', 'by', 'he', 'from', 'her', '\"', 'film', 'as', 'this', 'movie', 'their', 'but', 'one', 'at', 'about', 'the_NEG', 'a_NEG', 'to_NEG', 'are', \"there's\", '(', 'story', 'when', 'so', 'be', ',_NEG', ')', 'they', 'you', 'not', 'have', 'like', 'will', 'all', 'into', 'out', 'she', 'what', 'life', 'has', 'its', 'only', 'more', 'even', '--', ':', 'can', ';', 'home', 'look', \"it's\", 'if', 'where', 'most', 'him', 'search', 'but_NEG', 'love', 'both', 'make', 'begins', 'some', 'two', 'of_NEG', 'made', 'which', 'them']\n"
     ]
    }
   ],
   "source": [
    "unigram_feats = sentim_analyzer.unigram_word_feats(all_words_neg, min_freq=4)\n",
    "print(f'Length of unigram feats: {len(unigram_feats)}')\n",
    "print(unigram_feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f06f4e93-a319-452f-82f2-831fed2b0afa",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentim_analyzer.add_feat_extractor(extract_unigram_feats, unigrams=unigram_feats)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77d6edf7-b2f7-45cd-883c-d2adb1d19889",
   "metadata": {},
   "source": [
    "> We apply features to obtain a feature-value representation of our datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46cf3f57-0de3-47fa-b8f5-014afc39a221",
   "metadata": {},
   "source": [
    "## Training and results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "138b1731-82a1-4cd4-a1ce-f12b0533e68d",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set = sentim_analyzer.apply_features(training_docs)\n",
    "test_set = sentim_analyzer.apply_features(testing_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "627e837b-1047-4e29-af41-1b7908f26883",
   "metadata": {},
   "source": [
    "> We can now train our classifier on the training set, and subsequently output the evaluation results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "360f0d4d-6428-49a9-ac5e-da02ccf79956",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training classifier\n",
      "Evaluating NaiveBayesClassifier results...\n",
      "Accuracy: 0.8\n",
      "F-measure [obj]: 0.8\n",
      "F-measure [subj]: 0.8\n",
      "Precision [obj]: 0.8\n",
      "Precision [subj]: 0.8\n",
      "Recall [obj]: 0.8\n",
      "Recall [subj]: 0.8\n"
     ]
    }
   ],
   "source": [
    "trainer = NaiveBayesClassifier.train\n",
    "classifier = sentim_analyzer.train(trainer, training_set)\n",
    "                                       \n",
    "for key,value in sorted(sentim_analyzer.evaluate(test_set).items()):\n",
    "    print('{0}: {1}'.format(key, value))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "04ada52a-3c81-49b8-879e-e9a951c74930",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "e8dc55c8-a937-4409-9195-53dc093715dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [\"VADER is smart, handsome, and funny.\", # positive sentence example\n",
    "    \"VADER is smart, handsome, and funny!\", # punctuation emphasis handled correctly (sentiment intensity adjusted)\n",
    "    \"VADER is very smart, handsome, and funny.\",  # booster words handled correctly (sentiment intensity adjusted)\n",
    "    \"VADER is VERY SMART, handsome, and FUNNY.\",  # emphasis for ALLCAPS handled\n",
    "    \"VADER is VERY SMART, handsome, and FUNNY!!!\",# combination of signals - VADER appropriately adjusts intensity\n",
    "    \"VADER is VERY SMART, really handsome, and INCREDIBLY FUNNY!!!\",# booster words & punctuation make this close to ceiling for score\n",
    "    \"The book was good.\",         # positive sentence\n",
    "    \"The book was kind of good.\", # qualified positive sentence is handled correctly (intensity adjusted)\n",
    "    \"The plot was good, but the characters are uncompelling and the dialog is not great.\", # mixed negation sentence\n",
    "    \"A really bad, horrible book.\",       # negative sentence with booster words\n",
    "    \"At least it isn't a horrible book.\", # negated negative sentence with contraction\n",
    "    \":) and :D\",     # emoticons handled\n",
    "    \"\",              # an empty string is correctly handled\n",
    "    \"Today sux\",     #  negative slang handled\n",
    "    \"Today sux!\",    #  negative slang with punctuation emphasis handled\n",
    "    \"Today SUX!\",    #  negative slang with capitalization emphasis\n",
    "    \"Today kinda sux! But I'll get by, lol\" # mixed sentiment example with slang and constrastive conjunction \"but\"\n",
    "]\n",
    "paragraph = \"It was one of the worst movies I've seen, despite good reviews. \\\n",
    "    Unbelievably bad acting!! Poor direction. VERY poor production. \\\n",
    "    The movie was bad. Very bad movie. VERY bad movie. VERY BAD movie. VERY BAD movie!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "41fe99e9-cbe6-40c3-abaf-15b5ed7020d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import tokenize\n",
    "lines_list = tokenize.sent_tokenize(paragraph)\n",
    "sentences.extend(lines_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "7c84bb64-9b8f-44df-900e-dc7cb52708e5",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VADER is smart, handsome, and funny.\n",
      "compound: 0.8316, neg: 0.0, neu: 0.254, pos: 0.746, \n",
      "\n",
      "VADER is smart, handsome, and funny!\n",
      "compound: 0.8439, neg: 0.0, neu: 0.248, pos: 0.752, \n",
      "\n",
      "VADER is very smart, handsome, and funny.\n",
      "compound: 0.8545, neg: 0.0, neu: 0.299, pos: 0.701, \n",
      "\n",
      "VADER is VERY SMART, handsome, and FUNNY.\n",
      "compound: 0.9227, neg: 0.0, neu: 0.246, pos: 0.754, \n",
      "\n",
      "VADER is VERY SMART, handsome, and FUNNY!!!\n",
      "compound: 0.9342, neg: 0.0, neu: 0.233, pos: 0.767, \n",
      "\n",
      "VADER is VERY SMART, really handsome, and INCREDIBLY FUNNY!!!\n",
      "compound: 0.9469, neg: 0.0, neu: 0.294, pos: 0.706, \n",
      "\n",
      "The book was good.\n",
      "compound: 0.4404, neg: 0.0, neu: 0.508, pos: 0.492, \n",
      "\n",
      "The book was kind of good.\n",
      "compound: 0.3832, neg: 0.0, neu: 0.657, pos: 0.343, \n",
      "\n",
      "The plot was good, but the characters are uncompelling and the dialog is not great.\n",
      "compound: -0.7042, neg: 0.327, neu: 0.579, pos: 0.094, \n",
      "\n",
      "A really bad, horrible book.\n",
      "compound: -0.8211, neg: 0.791, neu: 0.209, pos: 0.0, \n",
      "\n",
      "At least it isn't a horrible book.\n",
      "compound: 0.431, neg: 0.0, neu: 0.637, pos: 0.363, \n",
      "\n",
      ":) and :D\n",
      "compound: 0.7925, neg: 0.0, neu: 0.124, pos: 0.876, \n",
      "\n",
      "\n",
      "compound: 0.0, neg: 0.0, neu: 0.0, pos: 0.0, \n",
      "\n",
      "Today sux\n",
      "compound: -0.3612, neg: 0.714, neu: 0.286, pos: 0.0, \n",
      "\n",
      "Today sux!\n",
      "compound: -0.4199, neg: 0.736, neu: 0.264, pos: 0.0, \n",
      "\n",
      "Today SUX!\n",
      "compound: -0.5461, neg: 0.779, neu: 0.221, pos: 0.0, \n",
      "\n",
      "Today kinda sux! But I'll get by, lol\n",
      "compound: 0.5249, neg: 0.138, neu: 0.517, pos: 0.344, \n",
      "\n",
      "It was one of the worst movies I've seen, despite good reviews.\n",
      "compound: -0.7584, neg: 0.394, neu: 0.606, pos: 0.0, \n",
      "\n",
      "Unbelievably bad acting!!\n",
      "compound: -0.6572, neg: 0.686, neu: 0.314, pos: 0.0, \n",
      "\n",
      "Poor direction.\n",
      "compound: -0.4767, neg: 0.756, neu: 0.244, pos: 0.0, \n",
      "\n",
      "VERY poor production.\n",
      "compound: -0.6281, neg: 0.674, neu: 0.326, pos: 0.0, \n",
      "\n",
      "The movie was bad.\n",
      "compound: -0.5423, neg: 0.538, neu: 0.462, pos: 0.0, \n",
      "\n",
      "Very bad movie.\n",
      "compound: -0.5849, neg: 0.655, neu: 0.345, pos: 0.0, \n",
      "\n",
      "VERY bad movie.\n",
      "compound: -0.6732, neg: 0.694, neu: 0.306, pos: 0.0, \n",
      "\n",
      "VERY BAD movie.\n",
      "compound: -0.7398, neg: 0.724, neu: 0.276, pos: 0.0, \n",
      "\n",
      "VERY BAD movie!\n",
      "compound: -0.7616, neg: 0.735, neu: 0.265, pos: 0.0, \n",
      "\n",
      "Most automated sentiment analysis tools are shit.\n",
      "compound: -0.5574, neg: 0.375, neu: 0.625, pos: 0.0, \n",
      "\n",
      "VADER sentiment analysis is the shit.\n",
      "compound: 0.6124, neg: 0.0, neu: 0.556, pos: 0.444, \n",
      "\n",
      "Sentiment analysis has never been good.\n",
      "compound: -0.3412, neg: 0.325, neu: 0.675, pos: 0.0, \n",
      "\n",
      "Sentiment analysis with VADER has never been this good.\n",
      "compound: 0.5228, neg: 0.0, neu: 0.703, pos: 0.297, \n",
      "\n",
      "Warren Beatty has never been so entertaining.\n",
      "compound: 0.5777, neg: 0.0, neu: 0.616, pos: 0.384, \n",
      "\n",
      "I won't say that the movie is astounding and I wouldn't claim that    the movie is too banal either.\n",
      "compound: 0.4215, neg: 0.0, neu: 0.851, pos: 0.149, \n",
      "\n",
      "I like to hate Michael Bay films, but I couldn't fault this one\n",
      "compound: 0.3153, neg: 0.157, neu: 0.534, pos: 0.309, \n",
      "\n",
      "I like to hate Michael Bay films, BUT I couldn't help but fault this one\n",
      "compound: -0.1531, neg: 0.277, neu: 0.477, pos: 0.246, \n",
      "\n",
      "It's one thing to watch an Uwe Boll film, but another thing entirely    to pay for it\n",
      "compound: -0.2541, neg: 0.112, neu: 0.888, pos: 0.0, \n",
      "\n",
      "The movie was too good\n",
      "compound: 0.4404, neg: 0.0, neu: 0.58, pos: 0.42, \n",
      "\n",
      "This movie was actually neither that funny, nor super witty.\n",
      "compound: -0.6759, neg: 0.41, neu: 0.59, pos: 0.0, \n",
      "\n",
      "This movie doesn't care about cleverness, wit or any other kind of    intelligent humor.\n",
      "compound: -0.1338, neg: 0.265, neu: 0.497, pos: 0.239, \n",
      "\n",
      "Those who find ugly meanings in beautiful things are corrupt without    being charming.\n",
      "compound: -0.3553, neg: 0.314, neu: 0.493, pos: 0.192, \n",
      "\n",
      "There are slow and repetitive parts, BUT it has just enough spice to    keep it interesting.\n",
      "compound: 0.4678, neg: 0.079, neu: 0.735, pos: 0.186, \n",
      "\n",
      "The script is not fantastic, but the acting is decent and the cinematography    is EXCELLENT!\n",
      "compound: 0.7565, neg: 0.092, neu: 0.607, pos: 0.301, \n",
      "\n",
      "Roger Dodger is one of the most compelling variations on this theme.\n",
      "compound: 0.2944, neg: 0.0, neu: 0.834, pos: 0.166, \n",
      "\n",
      "Roger Dodger is one of the least compelling variations on this theme.\n",
      "compound: -0.1695, neg: 0.132, neu: 0.868, pos: 0.0, \n",
      "\n",
      "Roger Dodger is at least compelling as a variation on the theme.\n",
      "compound: 0.2263, neg: 0.0, neu: 0.84, pos: 0.16, \n",
      "\n",
      "they fall in love with the product\n",
      "compound: 0.6369, neg: 0.0, neu: 0.588, pos: 0.412, \n",
      "\n",
      "but then it breaks\n",
      "compound: 0.0, neg: 0.0, neu: 1.0, pos: 0.0, \n",
      "\n",
      "usually around the time the 90 day warranty expires\n",
      "compound: 0.0, neg: 0.0, neu: 1.0, pos: 0.0, \n",
      "\n",
      "the twin towers collapsed today\n",
      "compound: -0.2732, neg: 0.344, neu: 0.656, pos: 0.0, \n",
      "\n",
      "However, Mr. Carter solemnly argues, his client carried out the kidnapping    under orders and in the ''least offensive way possible.''\n",
      "compound: -0.5859, neg: 0.23, neu: 0.697, pos: 0.074, \n",
      "\n",
      "Most automated sentiment analysis tools are shit.\n",
      "compound: -0.5574, neg: 0.375, neu: 0.625, pos: 0.0, \n",
      "\n",
      "VADER sentiment analysis is the shit.\n",
      "compound: 0.6124, neg: 0.0, neu: 0.556, pos: 0.444, \n",
      "\n",
      "Sentiment analysis has never been good.\n",
      "compound: -0.3412, neg: 0.325, neu: 0.675, pos: 0.0, \n",
      "\n",
      "Sentiment analysis with VADER has never been this good.\n",
      "compound: 0.5228, neg: 0.0, neu: 0.703, pos: 0.297, \n",
      "\n",
      "Warren Beatty has never been so entertaining.\n",
      "compound: 0.5777, neg: 0.0, neu: 0.616, pos: 0.384, \n",
      "\n",
      "I won't say that the movie is astounding and I wouldn't claim that    the movie is too banal either.\n",
      "compound: 0.4215, neg: 0.0, neu: 0.851, pos: 0.149, \n",
      "\n",
      "I like to hate Michael Bay films, but I couldn't fault this one\n",
      "compound: 0.3153, neg: 0.157, neu: 0.534, pos: 0.309, \n",
      "\n",
      "I like to hate Michael Bay films, BUT I couldn't help but fault this one\n",
      "compound: -0.1531, neg: 0.277, neu: 0.477, pos: 0.246, \n",
      "\n",
      "It's one thing to watch an Uwe Boll film, but another thing entirely    to pay for it\n",
      "compound: -0.2541, neg: 0.112, neu: 0.888, pos: 0.0, \n",
      "\n",
      "The movie was too good\n",
      "compound: 0.4404, neg: 0.0, neu: 0.58, pos: 0.42, \n",
      "\n",
      "This movie was actually neither that funny, nor super witty.\n",
      "compound: -0.6759, neg: 0.41, neu: 0.59, pos: 0.0, \n",
      "\n",
      "This movie doesn't care about cleverness, wit or any other kind of    intelligent humor.\n",
      "compound: -0.1338, neg: 0.265, neu: 0.497, pos: 0.239, \n",
      "\n",
      "Those who find ugly meanings in beautiful things are corrupt without    being charming.\n",
      "compound: -0.3553, neg: 0.314, neu: 0.493, pos: 0.192, \n",
      "\n",
      "There are slow and repetitive parts, BUT it has just enough spice to    keep it interesting.\n",
      "compound: 0.4678, neg: 0.079, neu: 0.735, pos: 0.186, \n",
      "\n",
      "The script is not fantastic, but the acting is decent and the cinematography    is EXCELLENT!\n",
      "compound: 0.7565, neg: 0.092, neu: 0.607, pos: 0.301, \n",
      "\n",
      "Roger Dodger is one of the most compelling variations on this theme.\n",
      "compound: 0.2944, neg: 0.0, neu: 0.834, pos: 0.166, \n",
      "\n",
      "Roger Dodger is one of the least compelling variations on this theme.\n",
      "compound: -0.1695, neg: 0.132, neu: 0.868, pos: 0.0, \n",
      "\n",
      "Roger Dodger is at least compelling as a variation on the theme.\n",
      "compound: 0.2263, neg: 0.0, neu: 0.84, pos: 0.16, \n",
      "\n",
      "they fall in love with the product\n",
      "compound: 0.6369, neg: 0.0, neu: 0.588, pos: 0.412, \n",
      "\n",
      "but then it breaks\n",
      "compound: 0.0, neg: 0.0, neu: 1.0, pos: 0.0, \n",
      "\n",
      "usually around the time the 90 day warranty expires\n",
      "compound: 0.0, neg: 0.0, neu: 1.0, pos: 0.0, \n",
      "\n",
      "the twin towers collapsed today\n",
      "compound: -0.2732, neg: 0.344, neu: 0.656, pos: 0.0, \n",
      "\n",
      "However, Mr. Carter solemnly argues, his client carried out the kidnapping    under orders and in the ''least offensive way possible.''\n",
      "compound: -0.5859, neg: 0.23, neu: 0.697, pos: 0.074, \n",
      "\n",
      "Most automated sentiment analysis tools are shit.\n",
      "compound: -0.5574, neg: 0.375, neu: 0.625, pos: 0.0, \n",
      "\n",
      "VADER sentiment analysis is the shit.\n",
      "compound: 0.6124, neg: 0.0, neu: 0.556, pos: 0.444, \n",
      "\n",
      "Sentiment analysis has never been good.\n",
      "compound: -0.3412, neg: 0.325, neu: 0.675, pos: 0.0, \n",
      "\n",
      "Sentiment analysis with VADER has never been this good.\n",
      "compound: 0.5228, neg: 0.0, neu: 0.703, pos: 0.297, \n",
      "\n",
      "Warren Beatty has never been so entertaining.\n",
      "compound: 0.5777, neg: 0.0, neu: 0.616, pos: 0.384, \n",
      "\n",
      "I won't say that the movie is astounding and I wouldn't claim that    the movie is too banal either.\n",
      "compound: 0.4215, neg: 0.0, neu: 0.851, pos: 0.149, \n",
      "\n",
      "I like to hate Michael Bay films, but I couldn't fault this one\n",
      "compound: 0.3153, neg: 0.157, neu: 0.534, pos: 0.309, \n",
      "\n",
      "I like to hate Michael Bay films, BUT I couldn't help but fault this one\n",
      "compound: -0.1531, neg: 0.277, neu: 0.477, pos: 0.246, \n",
      "\n",
      "It's one thing to watch an Uwe Boll film, but another thing entirely    to pay for it\n",
      "compound: -0.2541, neg: 0.112, neu: 0.888, pos: 0.0, \n",
      "\n",
      "The movie was too good\n",
      "compound: 0.4404, neg: 0.0, neu: 0.58, pos: 0.42, \n",
      "\n",
      "This movie was actually neither that funny, nor super witty.\n",
      "compound: -0.6759, neg: 0.41, neu: 0.59, pos: 0.0, \n",
      "\n",
      "This movie doesn't care about cleverness, wit or any other kind of    intelligent humor.\n",
      "compound: -0.1338, neg: 0.265, neu: 0.497, pos: 0.239, \n",
      "\n",
      "Those who find ugly meanings in beautiful things are corrupt without    being charming.\n",
      "compound: -0.3553, neg: 0.314, neu: 0.493, pos: 0.192, \n",
      "\n",
      "There are slow and repetitive parts, BUT it has just enough spice to    keep it interesting.\n",
      "compound: 0.4678, neg: 0.079, neu: 0.735, pos: 0.186, \n",
      "\n",
      "The script is not fantastic, but the acting is decent and the cinematography    is EXCELLENT!\n",
      "compound: 0.7565, neg: 0.092, neu: 0.607, pos: 0.301, \n",
      "\n",
      "Roger Dodger is one of the most compelling variations on this theme.\n",
      "compound: 0.2944, neg: 0.0, neu: 0.834, pos: 0.166, \n",
      "\n",
      "Roger Dodger is one of the least compelling variations on this theme.\n",
      "compound: -0.1695, neg: 0.132, neu: 0.868, pos: 0.0, \n",
      "\n",
      "Roger Dodger is at least compelling as a variation on the theme.\n",
      "compound: 0.2263, neg: 0.0, neu: 0.84, pos: 0.16, \n",
      "\n",
      "they fall in love with the product\n",
      "compound: 0.6369, neg: 0.0, neu: 0.588, pos: 0.412, \n",
      "\n",
      "but then it breaks\n",
      "compound: 0.0, neg: 0.0, neu: 1.0, pos: 0.0, \n",
      "\n",
      "usually around the time the 90 day warranty expires\n",
      "compound: 0.0, neg: 0.0, neu: 1.0, pos: 0.0, \n",
      "\n",
      "the twin towers collapsed today\n",
      "compound: -0.2732, neg: 0.344, neu: 0.656, pos: 0.0, \n",
      "\n",
      "However, Mr. Carter solemnly argues, his client carried out the kidnapping    under orders and in the ''least offensive way possible.''\n",
      "compound: -0.5859, neg: 0.23, neu: 0.697, pos: 0.074, \n",
      "\n"
     ]
    }
   ],
   "source": [
    "tricky_sentences = [\n",
    "   \"Most automated sentiment analysis tools are shit.\",\n",
    "   \"VADER sentiment analysis is the shit.\",\n",
    "   \"Sentiment analysis has never been good.\",\n",
    "   \"Sentiment analysis with VADER has never been this good.\",\n",
    "   \"Warren Beatty has never been so entertaining.\",\n",
    "   \"I won't say that the movie is astounding and I wouldn't claim that \\\n",
    "   the movie is too banal either.\",\n",
    "   \"I like to hate Michael Bay films, but I couldn't fault this one\",\n",
    "   \"I like to hate Michael Bay films, BUT I couldn't help but fault this one\",\n",
    "   \"It's one thing to watch an Uwe Boll film, but another thing entirely \\\n",
    "   to pay for it\",\n",
    "   \"The movie was too good\",\n",
    "   \"This movie was actually neither that funny, nor super witty.\",\n",
    "   \"This movie doesn't care about cleverness, wit or any other kind of \\\n",
    "   intelligent humor.\",\n",
    "   \"Those who find ugly meanings in beautiful things are corrupt without \\\n",
    "   being charming.\",\n",
    "   \"There are slow and repetitive parts, BUT it has just enough spice to \\\n",
    "   keep it interesting.\",\n",
    "   \"The script is not fantastic, but the acting is decent and the cinematography \\\n",
    "   is EXCELLENT!\",\n",
    "   \"Roger Dodger is one of the most compelling variations on this theme.\",\n",
    "   \"Roger Dodger is one of the least compelling variations on this theme.\",\n",
    "   \"Roger Dodger is at least compelling as a variation on the theme.\",\n",
    "   \"they fall in love with the product\",\n",
    "   \"but then it breaks\",\n",
    "   \"usually around the time the 90 day warranty expires\",\n",
    "   \"the twin towers collapsed today\",\n",
    "   \"However, Mr. Carter solemnly argues, his client carried out the kidnapping \\\n",
    "   under orders and in the ''least offensive way possible.''\"\n",
    "]\n",
    "\n",
    "sentences.extend(tricky_sentences)\n",
    "for sentence in sentences:\n",
    "    sid = SentimentIntensityAnalyzer()\n",
    "    print(sentence)\n",
    "    ss = sid.polarity_scores(sentence)\n",
    "    for k in sorted(ss):\n",
    "        print('{0}: {1}, '.format(k, ss[k]), end='')\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be5c79d4-8c07-41aa-b3c8-4f421d242cde",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e8e631e6-9c64-4991-b40d-35e84cc2aadd",
   "metadata": {},
   "source": [
    "# NLP - French POC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9944da2-1006-465e-b871-64670d620d42",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
